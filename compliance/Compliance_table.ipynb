{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd8e1c1-c7d7-42cb-a14d-76b9622f3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "df_env = pd.read_csv(\"../data/cleaned/right_environmental_data_2023_2025_final.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34987db3-d345-4839-831a-d5fb3ed7d7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8fa1639-4327-459e-8217-cb120daecbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy of the stacked environmental dataset\n",
    "# This protects the original dataframe from accidental modification\n",
    "df = df_env.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea556a61-5c67-492a-ba31-9b3ccff911ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all boolean *_recorded columns used to assess data completeness\n",
    "# Each column indicates whether a metric was reported for that row\n",
    "recorded_cols = [\n",
    "    \"Energy_kWh_recorded\", \n",
    "    \"Water_m3_recorded\", \n",
    "    \"Waste_tonnes_recorded\", \n",
    "    \"CO2_tonnes_recorded\", \n",
    "    \"Compliance_score_recorded\", \n",
    "    \"Environmental_incidents_recorded\", \n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a040b898-539a-40b8-b1c4-bb0d372203c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many metrics were recorded per Site–Year–Month\n",
    "df[\"recorded_count\"] = df[recorded_cols].sum(axis=1) \n",
    "\n",
    "# Define the total number of expected metrics per row\n",
    "# (Used as the denominator for completeness calculations)\n",
    "df[\"total_expected_metrics\"] = len(recorded_cols)\n",
    "\n",
    "# Calculate percentage data completeness for each record\n",
    "# Rounded to one decimal place for reporting clarity\n",
    "df[\"data_completness_pct\"] = ( \n",
    "    df[\"recorded_count\"] / df[\"total_expected_metrics\"] * 100 \n",
    ").round(1)\n",
    "\n",
    "# Calculate how many metrics are missing per record\n",
    "df[\"missing_metric_count\"] = (\n",
    "    df[\"total_expected_metrics\"] - df[\"recorded_count\"] \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982c861f-11b8-4f96-8c66-8f1f96dd3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag rows where at least one environmental incident occurred\n",
    "# NaN is treated as no incident\n",
    "df[\"incident_flag\"] = df[\"Environmental_incidents\"].fillna(0).gt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff0ac22-800b-47d3-bbab-c56d2517201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign compliance bands based on compliance score\n",
    "def compliance_band(score): \n",
    "    if pd.isna(score): \n",
    "        return \"Unknown\" \n",
    "    if score >= 80: \n",
    "        return \"Good\"\n",
    "    if score >= 60: \n",
    "        return \"Watch\" \n",
    "    return \"Poor\" \n",
    "\n",
    "df[\"compliance_band\"] = df[\"Compliance_score\"].apply(compliance_band) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25faddb4-172f-4985-95fb-856190f37320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline metric used for statistical variation detection\n",
    "baseline_metric = \"CO2_tonnes\" \n",
    "\n",
    "# Calculate site-level mean and standard deviation for baseline metric\n",
    "# Used to identify abnormal deviations within each site\n",
    "site_mean = df.groupby(\"Site\")[baseline_metric].transform(\"mean\")\n",
    "site_std = df.groupby(\"Site\")[baseline_metric].transform(\"std\").replace(0, np.nan)\n",
    "\n",
    "# Compute z-score for baseline metric per row\n",
    "df[\"variation_z\"] = (df[baseline_metric] - site_mean) / site_std\n",
    "\n",
    "# Flag records where variation exceeds ±2 standard deviations\n",
    "df[\"variation_flag\"] = df[\"variation_z\"].abs().ge(2).fillna(False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "360e951e-b8a3-4e05-b0de-89ec46f2e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine overall review status using hierarchical risk logic\n",
    "# Red = immediate concern\n",
    "# Amber = monitoring required\n",
    "# Green = no issues\n",
    "def review_status(row):\n",
    "    if row[\"incident_flag\"]:\n",
    "        return \"Red\"\n",
    "    if row[\"compliance_band\"] == \"Poor\":\n",
    "        return \"Red\"\n",
    "    if row[\"data_completness_pct\"] < 50:\n",
    "        return \"Red\"\n",
    "\n",
    "    if row[\"compliance_band\"] == \"Watch\":\n",
    "        return \"Amber\"\n",
    "    if row[\"data_completness_pct\"] < 80:\n",
    "        return \"Amber\"\n",
    "    if row[\"variation_flag\"]:\n",
    "        return \"Amber\"\n",
    "\n",
    "    return \"Green\"\n",
    "\n",
    "df[\"review_status\"] = df.apply(review_status, axis=1)\n",
    "\n",
    "# Flag whether follow-up action is required\n",
    "df[\"follow_up_required\"] = df[\"review_status\"].isin([\"Amber\", \"Red\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e25af3-6d88-4f5d-84aa-76d7c3487df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build human-readable review notes explaining why a record was flagged\n",
    "def build_notes(row):\n",
    "    reasons = []\n",
    "    if row[\"incident_flag\"]:\n",
    "        reasons.append(\"Incident recorded\")\n",
    "    if row[\"compliance_band\"] == \"Poor\":\n",
    "        reasons.append(\"Low compliance score\")\n",
    "    if row[\"compliance_band\"] == \"Watch\":\n",
    "        reasons.append(\"Moderate compliance score\")\n",
    "    if row[\"data_completness_pct\"] < 80:\n",
    "        reasons.append(f\"Incomplete monitoring ({row['data_completness_pct']}%)\")\n",
    "    if row[\"variation_flag\"] and not pd.isna(row[\"variation_z\"]):\n",
    "        reasons.append(\n",
    "            f\"Abnormal {baseline_metric} variation (z={row['variation_z']:.2f})\"\n",
    "        )\n",
    "    return \"; \".join(reasons) if reasons else \"No issues flagged\"\n",
    "\n",
    "df[\"review_notes\"] = df.apply(build_notes, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91ec5a34-86ef-4a22-a54a-67fdfbc0732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final compliance review table for reporting / SQL ingestion\n",
    "compliance_review = df[\n",
    "    [\n",
    "        \"env_id\",\n",
    "        \"Site\", \"Year\", \"Month\",\n",
    "        \"data_completness_pct\",\n",
    "        \"missing_metric_count\",\n",
    "        \"Environmental_incidents\",\n",
    "        \"incident_flag\",\n",
    "        \"Compliance_score\",\n",
    "        \"compliance_band\",\n",
    "        \"variation_flag\",\n",
    "        \"review_status\",\n",
    "        \"follow_up_required\",\n",
    "        \"review_notes\",\n",
    "    ]\n",
    "].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4dddbdb-2d48-46bf-bd37-dcee3b0a8f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort for readability and reporting consistency\n",
    "compliance_review = compliance_review.sort_values(\n",
    "    [\"Year\", \"Month\", \"Site\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Add a surrogate primary key for SQL usage\n",
    "compliance_review.insert(0, \"review_id\", range(1, len(compliance_review) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5f4fb06-699e-48ca-82dd-abba4a46c1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_status\n",
       "Red      102\n",
       "Green      9\n",
       "Amber      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks\n",
    "compliance_review[\"review_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f836210-88a1-48ac-b962-101b6a5db189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks\n",
    "compliance_review[\"env_id\"].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b2b5a27-1d46-46a8-bc14-29452f9a5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line standardises a column name after an earlier typo during feature \n",
    "# engineering, ensuring consistent naming before export and reporting\n",
    "df = df.rename(columns={\"data_completness_pct\": \"data_completeness_pct\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e456531a-8ed1-4315-9313-5c324b7ee71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export compliance review table\n",
  
    "compliance_review.to_csv(\"compliance_review.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_env_analytics)",
   "language": "python",
   "name": "env_env_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
